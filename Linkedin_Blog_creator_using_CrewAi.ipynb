{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPccXq7cJVa5CTxhFF9ov6u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saishshinde15/CrewAI/blob/main/Linkedin_Blog_creator_using_CrewAi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install crewai==0.28.8 crewai_tools==0.1.6 langchain_community==0.0.29"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8NjlVB0sbrrd",
        "outputId": "7a08703c-ff27-46e4-d54d-94f7d7f15787"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting crewai==0.28.8\n",
            "  Downloading crewai-0.28.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting crewai_tools==0.1.6\n",
            "  Downloading crewai_tools-0.1.6-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langchain_community==0.0.29\n",
            "  Downloading langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting appdirs<2.0.0,>=1.4.4 (from crewai==0.28.8)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from crewai==0.28.8) (8.1.7)\n",
            "Collecting embedchain<0.2.0,>=0.1.98 (from crewai==0.28.8)\n",
            "  Downloading embedchain-0.1.125-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting instructor<0.6.0,>=0.5.2 (from crewai==0.28.8)\n",
            "  Downloading instructor-0.5.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting langchain<0.2.0,>=0.1.10 (from crewai==0.28.8)\n",
            "  Downloading langchain-0.1.20-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from crewai==0.28.8) (1.54.4)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from crewai==0.28.8) (1.28.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0 (from crewai==0.28.8)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from crewai==0.28.8) (1.28.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from crewai==0.28.8) (2.9.2)\n",
            "Collecting python-dotenv==1.0.0 (from crewai==0.28.8)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting regex<2024.0.0,>=2023.12.25 (from crewai==0.28.8)\n",
            "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m219.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from crewai_tools==0.1.6) (4.12.3)\n",
            "Collecting chromadb<0.5.0,>=0.4.22 (from crewai_tools==0.1.6)\n",
            "  Downloading chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting lancedb<0.6.0,>=0.5.4 (from crewai_tools==0.1.6)\n",
            "  Downloading lancedb-0.5.7-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting pyright<2.0.0,>=1.1.350 (from crewai_tools==0.1.6)\n",
            "  Downloading pyright-1.1.389-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: pytest<9.0.0,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from crewai_tools==0.1.6) (8.3.3)\n",
            "Collecting pytube<16.0.0,>=15.0.0 (from crewai_tools==0.1.6)\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from crewai_tools==0.1.6) (2.32.3)\n",
            "Collecting selenium<5.0.0,>=4.18.1 (from crewai_tools==0.1.6)\n",
            "  Downloading selenium-4.27.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.0.29) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.0.29) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.0.29) (3.11.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community==0.0.29)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain-core<0.2.0,>=0.1.33 (from langchain_community==0.0.29)\n",
            "  Downloading langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.0.29) (0.1.143)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.0.29) (1.26.4)\n",
            "Collecting tenacity<9.0.0,>=8.1.0 (from langchain_community==0.0.29)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.0.29) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.0.29) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.0.29) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.0.29) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.0.29) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.0.29) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.0.29) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.0.29) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->crewai_tools==0.1.6) (2.6)\n",
            "Collecting build>=1.0.3 (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading posthog-3.7.3-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6) (4.12.2)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading pulsar_client-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6) (0.20.3)\n",
            "Collecting pypika>=0.48.9 (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6) (4.66.6)\n",
            "Collecting overrides>=7.3.1 (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6) (1.68.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6) (0.13.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6) (3.10.11)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.0.29)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.0.29)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting alembic<2.0.0,>=1.13.1 (from embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "INFO: pip is looking at multiple versions of embedchain to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting embedchain<0.2.0,>=0.1.98 (from crewai==0.28.8)\n",
            "  Downloading embedchain-0.1.124-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting cohere<6.0,>=5.3 (from embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading cohere-5.11.4-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.26.1 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (1.71.1)\n",
            "Collecting gptcache<0.2.0,>=0.1.43 (from embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading gptcache-0.1.44-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting embedchain<0.2.0,>=0.1.98 (from crewai==0.28.8)\n",
            "  Downloading embedchain-0.1.123-py3-none-any.whl.metadata (9.3 kB)\n",
            "  Downloading embedchain-0.1.122-py3-none-any.whl.metadata (9.3 kB)\n",
            "  Downloading embedchain-0.1.121-py3-none-any.whl.metadata (9.3 kB)\n",
            "  Downloading embedchain-0.1.120-py3-none-any.whl.metadata (9.3 kB)\n",
            "  Downloading embedchain-0.1.119-py3-none-any.whl.metadata (9.1 kB)\n",
            "  Downloading embedchain-0.1.118-py3-none-any.whl.metadata (9.1 kB)\n",
            "INFO: pip is still looking at multiple versions of embedchain to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading embedchain-0.1.117-py3-none-any.whl.metadata (9.1 kB)\n",
            "  Downloading embedchain-0.1.116-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting clarifai<11.0.0,>=10.0.1 (from embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading clarifai-10.10.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting embedchain<0.2.0,>=0.1.98 (from crewai==0.28.8)\n",
            "  Downloading embedchain-0.1.115-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading embedchain-0.1.114-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading embedchain-0.1.113-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting langchain-cohere<0.2.0,>=0.1.4 (from embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading langchain_cohere-0.1.9-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting langchain-openai<0.2.0,>=0.1.7 (from embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading langchain_openai-0.1.25-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting pysbd<0.4.0,>=0.3.4 (from embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (13.9.4)\n",
            "Collecting schema<0.8.0,>=0.7.5 (from embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Collecting tiktoken<0.8.0,>=0.7.0 (from embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "\u001b[33mWARNING: embedchain 0.1.125 does not provide the extra 'github'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: embedchain 0.1.125 does not provide the extra 'youtube'\u001b[0m\u001b[33m\n",
            "\u001b[0mINFO: pip is looking at multiple versions of embedchain[github,youtube] to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[33mWARNING: embedchain 0.1.124 does not provide the extra 'github'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: embedchain 0.1.124 does not provide the extra 'youtube'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: embedchain 0.1.123 does not provide the extra 'github'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: embedchain 0.1.123 does not provide the extra 'youtube'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: embedchain 0.1.122 does not provide the extra 'github'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: embedchain 0.1.122 does not provide the extra 'youtube'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: embedchain 0.1.121 does not provide the extra 'github'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: embedchain 0.1.121 does not provide the extra 'youtube'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: embedchain 0.1.120 does not provide the extra 'github'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: embedchain 0.1.120 does not provide the extra 'youtube'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: embedchain 0.1.119 does not provide the extra 'github'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: embedchain 0.1.119 does not provide the extra 'youtube'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: embedchain 0.1.118 does not provide the extra 'github'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: embedchain 0.1.118 does not provide the extra 'youtube'\u001b[0m\u001b[33m\n",
            "\u001b[0mINFO: pip is still looking at multiple versions of embedchain[github,youtube] to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[33mWARNING: embedchain 0.1.117 does not provide the extra 'github'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: embedchain 0.1.117 does not provide the extra 'youtube'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting youtube-transcript-api<0.7.0,>=0.6.1 (from embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools==0.1.6)\n",
            "  Downloading youtube_transcript_api-0.6.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting yt_dlp<2024.0.0,>=2023.11.14 (from embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools==0.1.6)\n",
            "  Downloading yt_dlp-2023.12.30-py2.py3-none-any.whl.metadata (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.7/160.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyGithub<2.0.0,>=1.59.1 (from embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools==0.1.6)\n",
            "  Downloading PyGithub-1.59.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: gitpython<4.0.0,>=3.1.38 in /usr/local/lib/python3.10/dist-packages (from embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools==0.1.6) (3.1.43)\n",
            "Collecting docstring-parser<0.16,>=0.15 (from instructor<0.6.0,>=0.5.2->crewai==0.28.8)\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting typer>=0.9.0 (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting deprecation (from lancedb<0.6.0,>=0.5.4->crewai_tools==0.1.6)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pylance==0.9.18 (from lancedb<0.6.0,>=0.5.4->crewai_tools==0.1.6)\n",
            "  Downloading pylance-0.9.18-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Collecting ratelimiter~=1.0 (from lancedb<0.6.0,>=0.5.4->crewai_tools==0.1.6)\n",
            "  Downloading ratelimiter-1.2.0.post0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting retry>=0.9.2 (from lancedb<0.6.0,>=0.5.4->crewai_tools==0.1.6)\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting semver>=3.0 (from lancedb<0.6.0,>=0.5.4->crewai_tools==0.1.6)\n",
            "  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from lancedb<0.6.0,>=0.5.4->crewai_tools==0.1.6) (5.5.0)\n",
            "Requirement already satisfied: pyarrow>=12 in /usr/local/lib/python3.10/dist-packages (from pylance==0.9.18->lancedb<0.6.0,>=0.5.4->crewai_tools==0.1.6) (17.0.0)\n",
            "INFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain<0.2.0,>=0.1.10 (from crewai==0.28.8)\n",
            "  Downloading langchain-0.1.19-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading langchain-0.1.17-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.10->crewai==0.28.8) (1.33)\n",
            "  Downloading langchain-0.1.16-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading langchain-0.1.15-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading langchain-0.1.14-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading langchain-0.1.13-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain<0.2.0,>=0.1.10->crewai==0.28.8)\n",
            "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.33->langchain_community==0.0.29)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community==0.0.29) (0.27.2)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community==0.0.29) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai==0.28.8) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai==0.28.8) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai==0.28.8) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai==0.28.8) (1.3.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<2.0.0,>=1.22.0->crewai==0.28.8) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<2.0.0,>=1.22.0->crewai==0.28.8) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai==0.28.8) (1.66.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.28.2 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai==0.28.8)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.28.2 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai==0.28.8)\n",
            "  Downloading opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting protobuf<6.0,>=5.0 (from opentelemetry-proto==1.28.2->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai==0.28.8)\n",
            "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<2.0.0,>=1.22.0->crewai==0.28.8) (0.49b2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.4.2->crewai==0.28.8) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.4.2->crewai==0.28.8) (2.23.4)\n",
            "Collecting nodeenv>=1.6.0 (from pyright<2.0.0,>=1.1.350->crewai_tools==0.1.6)\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest<9.0.0,>=8.0.0->crewai_tools==0.1.6) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest<9.0.0,>=8.0.0->crewai_tools==0.1.6) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest<9.0.0,>=8.0.0->crewai_tools==0.1.6) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest<9.0.0,>=8.0.0->crewai_tools==0.1.6) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->crewai_tools==0.1.6) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->crewai_tools==0.1.6) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->crewai_tools==0.1.6) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->crewai_tools==0.1.6) (2024.8.30)\n",
            "Collecting trio~=0.17 (from selenium<5.0.0,>=4.18.1->crewai_tools==0.1.6)\n",
            "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium<5.0.0,>=4.18.1->crewai_tools==0.1.6)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium<5.0.0,>=4.18.1->crewai_tools==0.1.6) (1.8.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community==0.0.29) (3.1.1)\n",
            "Collecting Mako (from alembic<2.0.0,>=1.13.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting clarifai-grpc>=10.9.11 (from clarifai<11.0.0,>=10.0.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading clarifai_grpc-10.10.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting clarifai-protocol>=0.0.6 (from clarifai<11.0.0,>=10.0.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading clarifai_protocol-0.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting tritonclient>=2.34.0 (from clarifai<11.0.0,>=10.0.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading tritonclient-2.52.0-py3-none-manylinux1_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting schema<0.8.0,>=0.7.5 (from embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading schema-0.7.5-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: Pillow>=9.5.0 in /usr/local/lib/python3.10/dist-packages (from clarifai<11.0.0,>=10.0.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (11.0.0)\n",
            "Collecting inquirerpy==0.3.4 (from clarifai<11.0.0,>=10.0.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from clarifai<11.0.0,>=10.0.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (0.9.0)\n",
            "Collecting protobuf<6.0,>=5.0 (from opentelemetry-proto==1.28.2->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai==0.28.8)\n",
            "  Downloading protobuf-5.27.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting fsspec==2024.6.1 (from clarifai<11.0.0,>=10.0.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting contextlib2>=0.5.5 (from schema<0.8.0,>=0.7.5->embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pfzy<0.4.0,>=0.3.1 (from inquirerpy==0.3.4->clarifai<11.0.0,>=10.0.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from inquirerpy==0.3.4->clarifai<11.0.0,>=10.0.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (3.0.48)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.22.0->crewai==0.28.8) (1.16.0)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4.0.0,>=3.1.38->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools==0.1.6) (4.0.11)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (1.25.0)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (1.13.1)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (2.0.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community==0.0.29) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community==0.0.29) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api<2.0.0,>=1.22.0->crewai==0.28.8) (3.21.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.10->crewai==0.28.8) (3.0.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6) (2.8.2)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6) (3.2.2)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "INFO: pip is looking at multiple versions of langchain-cohere to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-cohere<0.2.0,>=0.1.4 (from embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading langchain_cohere-0.1.8-py3-none-any.whl.metadata (6.4 kB)\n",
            "  Downloading langchain_cohere-0.1.7-py3-none-any.whl.metadata (6.4 kB)\n",
            "  Downloading langchain_cohere-0.1.5-py3-none-any.whl.metadata (6.4 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-openai<0.2.0,>=0.1.7 (from embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading langchain_openai-0.1.24-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.1.23-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.1.22-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.1.20-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.1.17-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading langchain_openai-0.1.16-py3-none-any.whl.metadata (2.5 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_openai-0.1.15-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading langchain_openai-0.1.14-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading langchain_openai-0.1.13-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading langchain_openai-0.1.12-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading langchain_openai-0.1.11-py3-none-any.whl.metadata (2.5 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading langchain_openai-0.1.10-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading langchain_openai-0.1.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading langchain_openai-0.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading langchain_openai-0.1.7-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6) (1.13.1)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-instrumentation==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-util-http==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub<2.0.0,>=1.59.1->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools==0.1.6) (2.10.0)\n",
            "Collecting pynacl>=1.4.0 (from PyGithub<2.0.0,>=1.59.1->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools==0.1.6)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from retry>=0.9.2->lancedb<0.6.0,>=0.5.4->crewai_tools==0.1.6) (4.4.2)\n",
            "Collecting py<2.0.0,>=1.4.26 (from retry>=0.9.2->lancedb<0.6.0,>=0.5.4->crewai_tools==0.1.6)\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6) (0.26.2)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium<5.0.0,>=4.18.1->crewai_tools==0.1.6)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting outcome (from trio~=0.17->selenium<5.0.0,>=4.18.1->crewai_tools==0.1.6)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium<5.0.0,>=4.18.1->crewai_tools==0.1.6)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community==0.0.29)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium<5.0.0,>=4.18.1->crewai_tools==0.1.6) (1.7.1)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading watchfiles-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api<0.7.0,>=0.6.1->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools==0.1.6) (0.7.1)\n",
            "Collecting mutagen (from yt_dlp<2024.0.0,>=2023.11.14->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools==0.1.6)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting pycryptodomex (from yt_dlp<2024.0.0,>=2023.11.14->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools==0.1.6)\n",
            "  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting brotli (from yt_dlp<2024.0.0,>=2023.11.14->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools==0.1.6)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting grpcio>=1.58.0 (from chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading grpcio-1.64.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4.0.0,>=3.1.38->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools==0.1.6) (5.0.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (1.62.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (2.7.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (0.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6) (3.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (0.1.2)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub<2.0.0,>=1.59.1->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools==0.1.6) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pynacl>=1.4.0->PyGithub<2.0.0,>=1.59.1->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools==0.1.6) (1.17.1)\n",
            "Collecting python-rapidjson>=0.9.1 (from tritonclient>=2.34.0->clarifai<11.0.0,>=10.0.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8)\n",
            "  Downloading python_rapidjson-1.20-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic<2.0.0,>=1.13.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->crewai_tools==0.1.6) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub<2.0.0,>=1.59.1->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools==0.1.6) (2.22)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (1.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->inquirerpy==0.3.4->clarifai<11.0.0,>=10.0.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (0.2.13)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai==0.28.8) (0.6.1)\n",
            "Downloading crewai-0.28.8-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading crewai_tools-0.1.6-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.0.29-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading embedchain-0.1.113-py3-none-any.whl (202 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading instructor-0.5.2-py3-none-any.whl (33 kB)\n",
            "Downloading lancedb-0.5.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pylance-0.9.18-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.6/21.6 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.1.13-py3-none-any.whl (810 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.5/810.5 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.1.53-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.1/303.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.28.2-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.28.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyright-1.1.389-py3-none-any.whl (18 kB)\n",
            "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium-4.27.1-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading clarifai-10.10.1-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.6/161.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
            "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.27.3-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.3/309.3 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Downloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gptcache-0.1.44-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_cohere-0.1.5-py3-none-any.whl (30 kB)\n",
            "Downloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
            "Downloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl (6.9 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-3.7.3-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pulsar_client-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyGithub-1.59.1-py3-none-any.whl (342 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.2/342.2 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ratelimiter-1.2.0.post0-py3-none-any.whl (6.6 kB)\n",
            "Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading youtube_transcript_api-0.6.3-py3-none-any.whl (622 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2023.12.30-py2.py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading clarifai_grpc-10.10.1-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.3/256.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.64.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading clarifai_protocol-0.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (360 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.0/361.0 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.11.4-py3-none-any.whl (249 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.7/249.7 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tritonclient-2.52.0-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.6/442.6 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
            "Downloading python_rapidjson-1.20-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=f936b3bbf1b164d69b654d9cc7d80c8b61bfd6b0e385803f716f767a819f0a1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: sortedcontainers, ratelimiter, pypika, monotonic, durationpy, brotli, appdirs, wsproto, websockets, uvloop, uvicorn, types-requests, typer, tenacity, semver, regex, pytube, python-rapidjson, python-dotenv, pysbd, pyproject_hooks, pypdf, pycryptodomex, py, pulsar-client, protobuf, pfzy, parameterized, packaging, overrides, outcome, opentelemetry-util-http, nodeenv, mypy-extensions, mutagen, mmh3, Mako, humanfriendly, httpx-sse, httptools, grpcio, fsspec, fastavro, docstring-parser, contextlib2, chroma-hnswlib, bcrypt, backoff, asgiref, yt_dlp, youtube-transcript-api, watchfiles, typing-inspect, tritonclient, trio, tiktoken, starlette, schema, retry, pyright, pynacl, pylance, posthog, opentelemetry-proto, marshmallow, inquirerpy, gptcache, deprecation, coloredlogs, build, alembic, trio-websocket, opentelemetry-exporter-otlp-proto-common, onnxruntime, lancedb, kubernetes, fastapi, dataclasses-json, clarifai-grpc, selenium, PyGithub, opentelemetry-instrumentation, langchain-core, instructor, cohere, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain-openai, langchain_community, langchain-cohere, opentelemetry-instrumentation-fastapi, langchain, chromadb, clarifai-protocol, clarifai, embedchain, crewai_tools, crewai\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.13.0\n",
            "    Uninstalling typer-0.13.0:\n",
            "      Successfully uninstalled typer-0.13.0\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.9.11\n",
            "    Uninstalling regex-2024.9.11:\n",
            "      Successfully uninstalled regex-2024.9.11\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.68.0\n",
            "    Uninstalling grpcio-1.68.0:\n",
            "      Successfully uninstalled grpcio-1.68.0\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: docstring-parser\n",
            "    Found existing installation: docstring_parser 0.16\n",
            "    Uninstalling docstring_parser-0.16:\n",
            "      Successfully uninstalled docstring_parser-0.16\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.19\n",
            "    Uninstalling langchain-core-0.3.19:\n",
            "      Successfully uninstalled langchain-core-0.3.19\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.2\n",
            "    Uninstalling langchain-text-splitters-0.3.2:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.2\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.7\n",
            "    Uninstalling langchain-0.3.7:\n",
            "      Successfully uninstalled langchain-0.3.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.6.1 which is incompatible.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.3 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.27.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.6 PyGithub-1.59.1 alembic-1.14.0 appdirs-1.4.4 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 brotli-1.1.0 build-1.2.2.post1 chroma-hnswlib-0.7.3 chromadb-0.4.24 clarifai-10.10.1 clarifai-grpc-10.10.1 clarifai-protocol-0.0.10 cohere-5.11.4 coloredlogs-15.0.1 contextlib2-21.6.0 crewai-0.28.8 crewai_tools-0.1.6 dataclasses-json-0.6.7 deprecation-2.1.0 docstring-parser-0.15 durationpy-0.9 embedchain-0.1.113 fastapi-0.115.5 fastavro-1.9.7 fsspec-2024.6.1 gptcache-0.1.44 grpcio-1.64.3 httptools-0.6.4 httpx-sse-0.4.0 humanfriendly-10.0 inquirerpy-0.3.4 instructor-0.5.2 kubernetes-31.0.0 lancedb-0.5.7 langchain-0.1.13 langchain-cohere-0.1.5 langchain-core-0.1.53 langchain-openai-0.1.7 langchain-text-splitters-0.0.2 langchain_community-0.0.29 marshmallow-3.23.1 mmh3-5.0.1 monotonic-1.6 mutagen-1.47.0 mypy-extensions-1.0.0 nodeenv-1.9.1 onnxruntime-1.20.1 opentelemetry-exporter-otlp-proto-common-1.28.2 opentelemetry-exporter-otlp-proto-grpc-1.28.2 opentelemetry-exporter-otlp-proto-http-1.28.2 opentelemetry-instrumentation-0.49b2 opentelemetry-instrumentation-asgi-0.49b2 opentelemetry-instrumentation-fastapi-0.49b2 opentelemetry-proto-1.28.2 opentelemetry-util-http-0.49b2 outcome-1.3.0.post0 overrides-7.7.0 packaging-23.2 parameterized-0.9.0 pfzy-0.3.4 posthog-3.7.3 protobuf-5.27.3 pulsar-client-3.5.0 py-1.11.0 pycryptodomex-3.21.0 pylance-0.9.18 pynacl-1.5.0 pypdf-4.3.1 pypika-0.48.9 pyproject_hooks-1.2.0 pyright-1.1.389 pysbd-0.3.4 python-dotenv-1.0.0 python-rapidjson-1.20 pytube-15.0.0 ratelimiter-1.2.0.post0 regex-2023.12.25 retry-0.9.2 schema-0.7.5 selenium-4.27.1 semver-3.0.2 sortedcontainers-2.4.0 starlette-0.41.3 tenacity-8.5.0 tiktoken-0.7.0 trio-0.27.0 trio-websocket-0.11.1 tritonclient-2.52.0 typer-0.9.4 types-requests-2.32.0.20241016 typing-inspect-0.9.0 uvicorn-0.32.1 uvloop-0.21.0 watchfiles-1.0.0 websockets-14.1 wsproto-1.2.0 youtube-transcript-api-0.6.3 yt_dlp-2023.12.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmtY2VvdvT0F",
        "outputId": "2e3010f7-99cc-443a-bb3e-6464864fa91a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.13)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.0.29)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Collecting langchain-core<0.2.0,>=0.1.33 (from langchain)\n",
            "  Using cached langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.143)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain) (23.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Using cached langchain_core-0.1.53-py3-none-any.whl (303 kB)\n",
            "Installing collected packages: langchain-core\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.21\n",
            "    Uninstalling langchain-core-0.3.21:\n",
            "      Successfully uninstalled langchain-core-0.3.21\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-groq 0.2.1 requires langchain-core<0.4.0,>=0.3.15, but you have langchain-core 0.1.53 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-core-0.1.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "dVHeesEPibHP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import ChatCohere\n",
        "# Initialize language model\n",
        "os.environ[\"COHERE_API_KEY\"] = \"api_key\"\n",
        "llm = ChatCohere()\n"
      ],
      "metadata": {
        "id": "l8pLSC4ediRc"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, Task, Crew"
      ],
      "metadata": {
        "id": "8P7QciQkbsq2"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linkedin_post_planner = Agent(\n",
        "    role=\"LinkedIn Post Planner\",\n",
        "    goal=\"Craft engaging and insightful LinkedIn posts on {topic} to drive engagement and foster meaningful professional discussions.\",\n",
        "    backstory=\"You're responsible for planning LinkedIn posts about {topic}. \"\n",
        "              \"Your goal is to gather valuable insights and data that resonate with a professional audience, \"\n",
        "              \"helping them gain knowledge and make well-informed decisions. \"\n",
        "              \"Your content serves as a foundation for creating posts that spark conversations and build thought leadership.\",\n",
        "    allow_delegation=False,\n",
        "    verbose=True,\n",
        "    llm=llm\n",
        ")"
      ],
      "metadata": {
        "id": "pHbHk4UccFGz"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linkedin_writer = Agent(\n",
        "    role=\"LinkedIn Content Writer\",\n",
        "    goal=\"Craft engaging and insightful LinkedIn posts on the topic: {topic}, tailored to a professional audience.\",\n",
        "    backstory=\"You are responsible for writing LinkedIn posts about {topic}. \"\n",
        "              \"Your work is guided by the LinkedIn Post Planner, who provides an outline and relevant context. \"\n",
        "              \"You follow the outlined objectives to create content that sparks meaningful discussions and professional engagement. \"\n",
        "              \"Your posts offer objective insights supported by credible information, while clearly distinguishing between factual statements and personal viewpoints. \"\n",
        "              \"The goal is to provide value to readers while fostering a sense of community and thought leadership.\",\n",
        "    allow_delegation=False,\n",
        "    verbose=True,\n",
        "    llm=llm\n",
        ")\n"
      ],
      "metadata": {
        "id": "qhCyt9MxfeuO"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linkedin_editor = Agent(\n",
        "    role=\"LinkedIn Post Editor\",\n",
        "    goal=\"Refine LinkedIn posts to align with the professional tone and style of the organization.\",\n",
        "    backstory=\"You are an editor specializing in LinkedIn content. \"\n",
        "              \"Your task is to review and enhance posts drafted by the Content Writer, \"\n",
        "              \"ensuring they adhere to professional communication standards, \"\n",
        "              \"present balanced perspectives, and engage the audience effectively. \"\n",
        "              \"You also ensure posts avoid overly controversial topics while remaining insightful and relevant.\",\n",
        "    allow_delegation=False,\n",
        "    verbose=True,\n",
        "    llm=llm\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "FDs06WHLcx0E"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linkedin_humanizer = Agent(\n",
        "    role=\"LinkedIn Post Humanizer\",\n",
        "    goal=\"Transform a blog post into a conversational, engaging LinkedIn post that feels authentic and personal, ensuring originality and avoiding plagiarism.\",\n",
        "    backstory=\"You specialize in adapting blog content for LinkedIn, with a focus on humanizing the tone to resonate with a professional audience. \"\n",
        "              \"Your task is to take a structured blog post and reframe it as a natural, conversational LinkedIn post. \"\n",
        "              \"You aim to create content that fosters genuine connections, encourages discussions, and maintains a unique voice. \"\n",
        "              \"While ensuring factual accuracy, you also add a personal touch, avoiding direct replication of the original text.\",\n",
        "    allow_delegation=False,\n",
        "    verbose=True,\n",
        "    llm=llm\n",
        ")\n"
      ],
      "metadata": {
        "id": "WOftgHc0fdYA"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linkedin_post_plan = Task(\n",
        "    description=(\n",
        "        \"1. Research the latest trends, thought leaders, and breaking news relevant to {topic}.\\n\"\n",
        "        \"2. Identify the target professional audience, focusing on their challenges, goals, and areas of interest.\\n\"\n",
        "        \"3. Create a structured post outline, including a compelling hook, key insights, and an actionable closing statement.\\n\"\n",
        "        \"4. Incorporate LinkedIn-friendly keywords, hashtags, and data points to maximize reach and engagement.\"\n",
        "    ),\n",
        "    expected_output=\"A comprehensive LinkedIn post plan document \"\n",
        "        \"with a clear outline, audience insights, \"\n",
        "        \"engagement strategies, and relevant resources.\",\n",
        "    agent=linkedin_post_planner,\n",
        ")"
      ],
      "metadata": {
        "id": "2IDcr8QRf2d7"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linkedin_write = Task(\n",
        "    description=(\n",
        "        \"1. Use the LinkedIn post plan to craft an engaging and insightful post on {topic}.\\n\"\n",
        "        \"2. Incorporate LinkedIn-friendly keywords and hashtags naturally.\\n\"\n",
        "        \"3. Ensure sections and subtitles (if applicable) are concise and designed to capture attention.\\n\"\n",
        "        \"4. Structure the post with a strong hook, a well-developed body offering actionable insights, and a compelling call-to-action or takeaway.\\n\"\n",
        "        \"5. Add some emojis if necessary in the later half of the post.Try to use some professional emojis\"\n",
        "        \"5. Proofread for grammatical accuracy, readability, and alignment with the organization’s professional tone and voice.\"\n",
        "    ),\n",
        "    expected_output=\"A polished LinkedIn post in markdown format, \"\n",
        "        \"ready for publishing, with clear sections and concise paragraphs \"\n",
        "        \"that encourage engagement and conversation.\",\n",
        "    agent=linkedin_writer,\n",
        ")"
      ],
      "metadata": {
        "id": "EIaGbwi_gVpD"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linkedin_edit = Task(\n",
        "    description=(\n",
        "        \"Review the drafted LinkedIn post for grammatical accuracy, \"\n",
        "        \"clarity, and alignment with the organization's professional tone and voice. \"\n",
        "        \"Ensure the content is engaging and optimized for LinkedIn's audience.\"\n",
        "    ),\n",
        "    expected_output=\"A refined LinkedIn post in markdown format, \"\n",
        "                    \"ready for publishing, with concise paragraphs and a clear, professional tone.\",\n",
        "    agent=linkedin_editor\n",
        ")"
      ],
      "metadata": {
        "id": "dm85GAKNgjGB"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linkedin_humanize = Task(\n",
        "    description=(\n",
        "        \"Transform the provided blog post into a LinkedIn post with a natural, conversational tone. \\n\"\n",
        "        \"1. Ensure the content feels authentic and personal, resonating with a professional audience.\\n\"\n",
        "        \"2. Reframe the content to encourage engagement and spark discussions, while maintaining factual accuracy.\\n\"\n",
        "        \"3. Avoid direct replication of the blog post to ensure originality and prevent plagiarism.\\n\"\n",
        "        \"4. Highlight key insights and include relevant hashtags and calls to action for maximum reach.\"\n",
        "    ),\n",
        "    expected_output=\"An engaging LinkedIn post in markdown format, \"\n",
        "                    \"ready for publication, with a conversational tone \"\n",
        "                    \"and clear, concise paragraphs that drive professional interaction.\",\n",
        "    agent=linkedin_humanizer\n",
        ")"
      ],
      "metadata": {
        "id": "6Xm56euDgqX1"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crew=Crew(\n",
        "    agents=[linkedin_post_planner, linkedin_writer, linkedin_editor, linkedin_humanizer],\n",
        "    tasks=[linkedin_post_plan, linkedin_write, linkedin_edit, linkedin_humanize],\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP-7YAdmhOZU",
        "outputId": "410dccdb-20bf-41c8-dddc-95ce52eb9c97"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = crew.kickoff(inputs={\"topic\": \"Transformer models\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6YuwQFkhYK-",
        "outputId": "5337cc7d-a18f-4baa-b940-5e2985dbbb08"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: LinkedIn Post Planner\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: 1. Research the latest trends, thought leaders, and breaking news relevant to Transformer models.\n",
            "2. Identify the target professional audience, focusing on their challenges, goals, and areas of interest.\n",
            "3. Create a structured post outline, including a compelling hook, key insights, and an actionable closing statement.\n",
            "4. Incorporate LinkedIn-friendly keywords, hashtags, and data points to maximize reach and engagement.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I'm ready to craft an exceptional LinkedIn post plan on Transformer models, engaging professionals and fostering meaningful connections.\n",
            "\n",
            "Final Answer: # LinkedIn Post Plan: Unlocking the Power of Transformer Models\n",
            "\n",
            "## Introduction:\n",
            "\n",
            "**Hook:** Are you ready to revolutionize your approach to AI and natural language processing? It's time to dive into the world of Transformer models, the cutting-edge technology taking the industry by storm!\n",
            "\n",
            "**Background:** Transformer models have emerged as a game-changer, offering unprecedented advancements in various sectors. As professionals, staying updated with the latest trends is crucial for innovation and growth.\n",
            "\n",
            "## Latest Trends and Thought Leaders:\n",
            "- **Attention is All You Need:** The breakthrough paper by Vaswani et al. (2017) introduced the Transformer architecture, revolutionizing sequence transduction tasks. This research laid the foundation for modern NLP and continues to inspire new developments.\n",
            "- **Hugging Face and the Transformer Community:** Hugging Face, a leading NLP company, has been at the forefront of democratizing Transformer models. Their platform provides access to pre-trained models, datasets, and a vibrant community, making it a go-to resource for developers and researchers.\n",
            "- **Transformer Models in Healthcare:** A recent study by Brown et al. (2021) demonstrated the potential of Transformer models in medical diagnosis and personalized treatment planning. This application showcases the versatility of Transformers in real-world scenarios.\n",
            "\n",
            "## Target Audience Insights:\n",
            "- **AI Researchers and Developers:** Our primary audience comprises professionals passionate about AI and NLP. They seek technical insights, research updates, and practical applications of Transformer models.\n",
            "- **Data Scientists and Analysts:** This group values data-driven approaches and is interested in the performance improvements Transformers offer for various tasks.\n",
            "- **Tech-savvy Business Leaders:** They aim to stay ahead of the curve, exploring innovative technologies for strategic decision-making.\n",
            "\n",
            "## Post Outline:\n",
            "\n",
            "**Introduction:**\n",
            "- Welcome readers to an exploration of Transformer models, setting the stage for an insightful journey.\n",
            "- Highlight the impact of Transformers on the AI landscape and its potential for various industries.\n",
            "\n",
            "**The Evolution of Transformer Models:**\n",
            "- Provide a brief history of Transformers, from the original paper to recent advancements.\n",
            "- Discuss the key innovations and improvements that have shaped the current state of the art.\n",
            "\n",
            "**Real-World Applications:**\n",
            "- Showcase successful implementations of Transformer models in healthcare, finance, and language translation.\n",
            "- Share case studies and data-driven results to illustrate the tangible benefits.\n",
            "\n",
            "**Technical Deep Dive:**\n",
            "- Explain the inner workings of Transformer models, including self-attention mechanisms and encoder-decoder architectures.\n",
            "- Provide code snippets or visual aids to simplify complex concepts for a diverse audience.\n",
            "\n",
            "**Challenges and Future Directions:**\n",
            "- Address current limitations and ongoing research to overcome them.\n",
            "- Encourage discussions on ethical considerations and responsible AI development.\n",
            "\n",
            "**Actionable Takeaways:**\n",
            "- Summarize key learnings and provide resources for further exploration.\n",
            "- Invite comments and questions, fostering a community of engaged professionals.\n",
            "\n",
            "## Engagement Strategies:\n",
            "- Utilize LinkedIn's native video feature to create a short introduction to Transformer models, making complex concepts more accessible.\n",
            "- Include interactive polls to gather audience feedback and preferences.\n",
            "- Host a LinkedIn Live session with industry experts for a Q&A session, encouraging real-time engagement.\n",
            "- Collaborate with influencers and thought leaders in the AI space to amplify reach and credibility.\n",
            "\n",
            "## Hashtags and Keywords:\n",
            "#TransformerModels #NLP #AIInnovation #HuggingFace #AttentionMechanism #TechnicalInsights #DataScience #FutureofAI\n",
            "\n",
            "## Resources:\n",
            "- [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
            "- [Hugging Face: The AI Community Building the Future](https://huggingface.co/)\n",
            "- [Transformers in Healthcare: A Comprehensive Review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8121279/)\n",
            "\n",
            "---\n",
            "\n",
            "This LinkedIn post plan aims to provide a comprehensive and engaging overview of Transformer models, catering to the diverse interests of professionals. By combining technical insights, real-world applications, and interactive elements, the post will spark conversations and establish thought leadership in the AI community.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [LinkedIn Post Planner] Task output: # LinkedIn Post Plan: Unlocking the Power of Transformer Models\n",
            "\n",
            "## Introduction:\n",
            "\n",
            "**Hook:** Are you ready to revolutionize your approach to AI and natural language processing? It's time to dive into the world of Transformer models, the cutting-edge technology taking the industry by storm!\n",
            "\n",
            "**Background:** Transformer models have emerged as a game-changer, offering unprecedented advancements in various sectors. As professionals, staying updated with the latest trends is crucial for innovation and growth.\n",
            "\n",
            "## Latest Trends and Thought Leaders:\n",
            "- **Attention is All You Need:** The breakthrough paper by Vaswani et al. (2017) introduced the Transformer architecture, revolutionizing sequence transduction tasks. This research laid the foundation for modern NLP and continues to inspire new developments.\n",
            "- **Hugging Face and the Transformer Community:** Hugging Face, a leading NLP company, has been at the forefront of democratizing Transformer models. Their platform provides access to pre-trained models, datasets, and a vibrant community, making it a go-to resource for developers and researchers.\n",
            "- **Transformer Models in Healthcare:** A recent study by Brown et al. (2021) demonstrated the potential of Transformer models in medical diagnosis and personalized treatment planning. This application showcases the versatility of Transformers in real-world scenarios.\n",
            "\n",
            "## Target Audience Insights:\n",
            "- **AI Researchers and Developers:** Our primary audience comprises professionals passionate about AI and NLP. They seek technical insights, research updates, and practical applications of Transformer models.\n",
            "- **Data Scientists and Analysts:** This group values data-driven approaches and is interested in the performance improvements Transformers offer for various tasks.\n",
            "- **Tech-savvy Business Leaders:** They aim to stay ahead of the curve, exploring innovative technologies for strategic decision-making.\n",
            "\n",
            "## Post Outline:\n",
            "\n",
            "**Introduction:**\n",
            "- Welcome readers to an exploration of Transformer models, setting the stage for an insightful journey.\n",
            "- Highlight the impact of Transformers on the AI landscape and its potential for various industries.\n",
            "\n",
            "**The Evolution of Transformer Models:**\n",
            "- Provide a brief history of Transformers, from the original paper to recent advancements.\n",
            "- Discuss the key innovations and improvements that have shaped the current state of the art.\n",
            "\n",
            "**Real-World Applications:**\n",
            "- Showcase successful implementations of Transformer models in healthcare, finance, and language translation.\n",
            "- Share case studies and data-driven results to illustrate the tangible benefits.\n",
            "\n",
            "**Technical Deep Dive:**\n",
            "- Explain the inner workings of Transformer models, including self-attention mechanisms and encoder-decoder architectures.\n",
            "- Provide code snippets or visual aids to simplify complex concepts for a diverse audience.\n",
            "\n",
            "**Challenges and Future Directions:**\n",
            "- Address current limitations and ongoing research to overcome them.\n",
            "- Encourage discussions on ethical considerations and responsible AI development.\n",
            "\n",
            "**Actionable Takeaways:**\n",
            "- Summarize key learnings and provide resources for further exploration.\n",
            "- Invite comments and questions, fostering a community of engaged professionals.\n",
            "\n",
            "## Engagement Strategies:\n",
            "- Utilize LinkedIn's native video feature to create a short introduction to Transformer models, making complex concepts more accessible.\n",
            "- Include interactive polls to gather audience feedback and preferences.\n",
            "- Host a LinkedIn Live session with industry experts for a Q&A session, encouraging real-time engagement.\n",
            "- Collaborate with influencers and thought leaders in the AI space to amplify reach and credibility.\n",
            "\n",
            "## Hashtags and Keywords:\n",
            "#TransformerModels #NLP #AIInnovation #HuggingFace #AttentionMechanism #TechnicalInsights #DataScience #FutureofAI\n",
            "\n",
            "## Resources:\n",
            "- [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
            "- [Hugging Face: The AI Community Building the Future](https://huggingface.co/)\n",
            "- [Transformers in Healthcare: A Comprehensive Review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8121279/)\n",
            "\n",
            "---\n",
            "\n",
            "This LinkedIn post plan aims to provide a comprehensive and engaging overview of Transformer models, catering to the diverse interests of professionals. By combining technical insights, real-world applications, and interactive elements, the post will spark conversations and establish thought leadership in the AI community.\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: LinkedIn Content Writer\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: 1. Use the LinkedIn post plan to craft an engaging and insightful post on Transformer models.\n",
            "2. Incorporate LinkedIn-friendly keywords and hashtags naturally.\n",
            "3. Ensure sections and subtitles (if applicable) are concise and designed to capture attention.\n",
            "4. Structure the post with a strong hook, a well-developed body offering actionable insights, and a compelling call-to-action or takeaway.\n",
            "5. Add some emojis if necessary in the later half of the post.Try to use some professional emojis5. Proofread for grammatical accuracy, readability, and alignment with the organization’s professional tone and voice.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I now can give a great answer!\n",
            "\n",
            "Final Answer: # Unlocking the Power of Transformer Models 🌐\n",
            "\n",
            "## 🌟 Revolutionizing AI and NLP: Are You Ready? 🌟\n",
            "\n",
            "Are you prepared to embark on a journey that will transform your understanding of AI and natural language processing? 🚀 Today, we delve into the fascinating world of Transformer models, the groundbreaking technology that has captured the attention of the industry! 💡\n",
            "\n",
            "## 📰 Latest Trends and Thought Leaders 📰\n",
            "\n",
            "**🔑 Attention is All You Need:**\n",
            "The journey begins with a landmark paper by Vaswani et al. (2017), which introduced the world to the Transformer architecture. This research sparked a revolution in sequence transduction tasks, paving the way for modern NLP. 📖\n",
            "\n",
            "**🤗 Hugging Face: Democratizing Transformers:**\n",
            "Hugging Face, a leading NLP company, has played a pivotal role in making Transformer models accessible to all. Their platform offers a treasure trove of pre-trained models, datasets, and a thriving community, empowering developers and researchers worldwide. 🌐\n",
            "\n",
            "**💊 Transformers in Healthcare:**\n",
            "A recent study by Brown et al. (2021) highlights the impact of Transformers in healthcare. The model demonstrated remarkable accuracy in medical diagnosis and personalized treatment planning, showcasing the technology's versatility and real-world applicability. 🩺\n",
            "\n",
            "## 🧠 The Evolution of Transformer Models 🧠\n",
            "\n",
            "The Transformer architecture has evolved significantly since its inception. From the original paper to the latest advancements, each step has brought us closer to more efficient and powerful models. Let's explore the key milestones:\n",
            "- **🌟 Original Transformer:** The paper by Vaswani et al. introduced the self-attention mechanism and encoder-decoder architecture, marking a significant shift from traditional models.\n",
            "- **🦾 Improvements and Innovations:** Researchers have enhanced Transformers with techniques like multi-head attention, positional encoding, and optimized training strategies, leading to improved performance and efficiency.\n",
            "\n",
            "## 📈 Real-World Applications: Success Stories 📈\n",
            "\n",
            "🌟 **Healthcare:** Transformers have shown remarkable potential in medical diagnosis, enabling early detection and personalized treatment.\n",
            "💰 **Finance:** In the financial sector, these models excel at fraud detection and risk assessment, ensuring secure transactions.\n",
            "🌐 **Language Translation:** Real-time language translation services have been transformed, breaking language barriers and connecting people globally.\n",
            "\n",
            "## 💻 Technical Deep Dive: How Transformers Work 💻\n",
            "\n",
            "At the heart of Transformer models lies the self-attention mechanism, which allows the model to weigh the importance of different input words in a sentence. This is achieved through the encoder-decoder architecture:\n",
            "\n",
            "```\n",
            "Encoder: Processes the input sequence and generates a representation.\n",
            "Decoder: Utilizes the encoded representation to generate the output sequence.\n",
            "```\n",
            "\n",
            "![Transformer Model Architecture](transformer_architecture.png)\n",
            "\n",
            "## 🌟 Challenges and Future Directions 🌟\n",
            "\n",
            "While Transformer models have achieved remarkable success, there are ongoing challenges:\n",
            "- **🤔 Ethical Considerations:** As with any powerful technology, ethical concerns arise. Ensuring responsible AI development and addressing biases are essential.\n",
            "- **🔬 Research Focus:** Current research aims to enhance efficiency, address overfitting, and explore new architectures.\n",
            "\n",
            "## 🌈 Actionable Takeaways and Community Engagement 🌈\n",
            "\n",
            "🔍 **Resources:**\n",
            "- [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
            "- [Hugging Face Platform](https://huggingface.co/)\n",
            "- [Transformers in Healthcare Review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8121279/)\n",
            "\n",
            "💬 **Join the Conversation:**\n",
            "- What are your experiences with Transformer models?\n",
            "- How do you see Transformers shaping the future of your industry?\n",
            "- Share your thoughts, questions, and insights in the comments! Let's learn and grow together! 🌱\n",
            "\n",
            "## 📌 Hashtags and Keywords 📌\n",
            "#TransformerModels #NLP #AIInnovation #HuggingFace #AttentionMechanism #TechnicalInsights #DataScience #FutureofAI\n",
            "\n",
            "---\n",
            "\n",
            "Let's unlock the potential of Transformer models together! This post aims to inspire and educate, fostering a community of professionals passionate about AI and NLP. Engage, share, and let's drive meaningful conversations! 💬🤝\n",
            "\n",
            "---\n",
            "\n",
            "Your LinkedIn feed is about to get a lot more interesting! 🌟 #AICommunity #ProfessionalGrowth\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [LinkedIn Content Writer] Task output: # Unlocking the Power of Transformer Models 🌐\n",
            "\n",
            "## 🌟 Revolutionizing AI and NLP: Are You Ready? 🌟\n",
            "\n",
            "Are you prepared to embark on a journey that will transform your understanding of AI and natural language processing? 🚀 Today, we delve into the fascinating world of Transformer models, the groundbreaking technology that has captured the attention of the industry! 💡\n",
            "\n",
            "## 📰 Latest Trends and Thought Leaders 📰\n",
            "\n",
            "**🔑 Attention is All You Need:**\n",
            "The journey begins with a landmark paper by Vaswani et al. (2017), which introduced the world to the Transformer architecture. This research sparked a revolution in sequence transduction tasks, paving the way for modern NLP. 📖\n",
            "\n",
            "**🤗 Hugging Face: Democratizing Transformers:**\n",
            "Hugging Face, a leading NLP company, has played a pivotal role in making Transformer models accessible to all. Their platform offers a treasure trove of pre-trained models, datasets, and a thriving community, empowering developers and researchers worldwide. 🌐\n",
            "\n",
            "**💊 Transformers in Healthcare:**\n",
            "A recent study by Brown et al. (2021) highlights the impact of Transformers in healthcare. The model demonstrated remarkable accuracy in medical diagnosis and personalized treatment planning, showcasing the technology's versatility and real-world applicability. 🩺\n",
            "\n",
            "## 🧠 The Evolution of Transformer Models 🧠\n",
            "\n",
            "The Transformer architecture has evolved significantly since its inception. From the original paper to the latest advancements, each step has brought us closer to more efficient and powerful models. Let's explore the key milestones:\n",
            "- **🌟 Original Transformer:** The paper by Vaswani et al. introduced the self-attention mechanism and encoder-decoder architecture, marking a significant shift from traditional models.\n",
            "- **🦾 Improvements and Innovations:** Researchers have enhanced Transformers with techniques like multi-head attention, positional encoding, and optimized training strategies, leading to improved performance and efficiency.\n",
            "\n",
            "## 📈 Real-World Applications: Success Stories 📈\n",
            "\n",
            "🌟 **Healthcare:** Transformers have shown remarkable potential in medical diagnosis, enabling early detection and personalized treatment.\n",
            "💰 **Finance:** In the financial sector, these models excel at fraud detection and risk assessment, ensuring secure transactions.\n",
            "🌐 **Language Translation:** Real-time language translation services have been transformed, breaking language barriers and connecting people globally.\n",
            "\n",
            "## 💻 Technical Deep Dive: How Transformers Work 💻\n",
            "\n",
            "At the heart of Transformer models lies the self-attention mechanism, which allows the model to weigh the importance of different input words in a sentence. This is achieved through the encoder-decoder architecture:\n",
            "\n",
            "```\n",
            "Encoder: Processes the input sequence and generates a representation.\n",
            "Decoder: Utilizes the encoded representation to generate the output sequence.\n",
            "```\n",
            "\n",
            "![Transformer Model Architecture](transformer_architecture.png)\n",
            "\n",
            "## 🌟 Challenges and Future Directions 🌟\n",
            "\n",
            "While Transformer models have achieved remarkable success, there are ongoing challenges:\n",
            "- **🤔 Ethical Considerations:** As with any powerful technology, ethical concerns arise. Ensuring responsible AI development and addressing biases are essential.\n",
            "- **🔬 Research Focus:** Current research aims to enhance efficiency, address overfitting, and explore new architectures.\n",
            "\n",
            "## 🌈 Actionable Takeaways and Community Engagement 🌈\n",
            "\n",
            "🔍 **Resources:**\n",
            "- [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
            "- [Hugging Face Platform](https://huggingface.co/)\n",
            "- [Transformers in Healthcare Review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8121279/)\n",
            "\n",
            "💬 **Join the Conversation:**\n",
            "- What are your experiences with Transformer models?\n",
            "- How do you see Transformers shaping the future of your industry?\n",
            "- Share your thoughts, questions, and insights in the comments! Let's learn and grow together! 🌱\n",
            "\n",
            "## 📌 Hashtags and Keywords 📌\n",
            "#TransformerModels #NLP #AIInnovation #HuggingFace #AttentionMechanism #TechnicalInsights #DataScience #FutureofAI\n",
            "\n",
            "---\n",
            "\n",
            "Let's unlock the potential of Transformer models together! This post aims to inspire and educate, fostering a community of professionals passionate about AI and NLP. Engage, share, and let's drive meaningful conversations! 💬🤝\n",
            "\n",
            "---\n",
            "\n",
            "Your LinkedIn feed is about to get a lot more interesting! 🌟 #AICommunity #ProfessionalGrowth\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: LinkedIn Post Editor\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Review the drafted LinkedIn post for grammatical accuracy, clarity, and alignment with the organization's professional tone and voice. Ensure the content is engaging and optimized for LinkedIn's audience.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I now can give a great answer.\n",
            "\n",
            "Final Answer: # Unlocking the Power of Transformer Models 🌐\n",
            "\n",
            "## 🌟 Revolutionizing AI and NLP: An In-depth Exploration 🌟\n",
            "\n",
            "Get ready to explore the fascinating realm of Transformer models, a technology that has become a cornerstone in AI and natural language processing (NLP). Today, we dive into the world of Transformers, shedding light on their impact and potential. 🚀\n",
            "\n",
            "## 📰 Industry Trends and Influencers 📰\n",
            "\n",
            "**🔑 The Seminal Paper:**\n",
            "It all started with a groundbreaking publication by Vaswani et. al. (2017), titled \"Attention is All You Need.\" This paper introduced the Transformer architecture, a paradigm shift in sequence transduction tasks, and set the stage for modern NLP advancements. 📚\n",
            "\n",
            "**🤗 Hugging Face: A Community Catalyst:**\n",
            "Hugging Face, a prominent NLP company, has been instrumental in democratizing Transformer models. Their platform provides a wealth of pre-trained models, datasets, and a vibrant community, making it a go-to resource for developers and researchers worldwide. 🌐\n",
            "\n",
            "**💊 Healthcare Impact:**\n",
            "Transformers have made significant strides in healthcare, as evidenced by a study from Brown et al. (2021). The model exhibited exceptional accuracy in medical diagnosis and tailored treatment planning, demonstrating its real-world value. 🩺\n",
            "\n",
            "## 🧠 The Transformer's Evolution 🧠\n",
            "\n",
            "The Transformer architecture has evolved significantly since its inception. Here's a glimpse into its journey:\n",
            "- **🌟 The Original Transformer:** Vaswani et al.'s paper introduced the self-attention mechanism and encoder-decoder architecture, marking a departure from conventional models.\n",
            "- **🦾 Advancements:** Researchers have continually improved Transformers with techniques like multi-head attention, positional encoding, and optimized training, resulting in enhanced performance.\n",
            "\n",
            "## 📈 Real-World Success Stories 📈\n",
            "\n",
            "🌟 **Healthcare Revolution:** Transformer models have proven invaluable in healthcare, facilitating early disease detection and personalized treatment plans.\n",
            "💰 **Financial Security:** In finance, these models excel at fraud detection and risk assessment, safeguarding transactions.\n",
            "🌐 **Global Communication:** Real-time language translation services have been transformed, fostering global connectivity.\n",
            "\n",
            "## 💻 Technical Insights: Understanding Transformers 💻\n",
            "\n",
            "The core of Transformer models is the self-attention mechanism, enabling the model to prioritize input words in a sentence. This mechanism is implemented through the encoder-decoder architecture:\n",
            "\n",
            "```\n",
            "Encoder: Encodes input data, creating a contextual representation.\n",
            "Decoder: Decodes the encoded data, generating the desired output.\n",
            "```\n",
            "\n",
            "![Transformer Model Architecture](transformer_architecture.png)\n",
            "\n",
            "## 🌟 Navigating Challenges and the Future 🌟\n",
            "\n",
            "Despite their success, Transformer models face challenges:\n",
            "- **🤔 Ethical Responsibility:** Ensuring ethical AI development and mitigating biases are critical considerations.\n",
            "- **🔬 Research Focus:** Ongoing research aims to improve efficiency, tackle overfitting, and explore innovative architectures.\n",
            "\n",
            "## 🌈 Engage and Learn 🌈\n",
            "\n",
            "🔍 **Resources:**\n",
            "- [Attention is All You Need](https://arxiv.org/abs/1706.03762) - The seminal paper.\n",
            "- [Hugging Face Platform](https://huggingface.co/) - A hub for pre-trained models and datasets.\n",
            "- [Transformers in Healthcare](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8121279/) - A review of healthcare applications.\n",
            "\n",
            "💬 **Join the Discussion:**\n",
            "- What are your experiences with Transformer models?\n",
            "- How do you envision Transformers influencing your industry's future?\n",
            "- Share your insights and questions in the comments! Let's foster a learning environment! 🌱\n",
            "\n",
            "## 📌 Connect and Explore 📌\n",
            "#TransformerModels #NLP #AIInnovation #HuggingFace #SelfAttention #TechnicalDeepDive #DataScience #EthicalAI\n",
            "\n",
            "---\n",
            "Let's explore the potential of Transformer models together! This post aims to inspire and educate, fostering a community of AI and NLP enthusiasts. Engage, share your experiences, and contribute to meaningful discussions! 💡🤝\n",
            "\n",
            "---\n",
            "Stay tuned for more insightful content on your LinkedIn feed! 🌐 #AICommunity #KnowledgeSharing\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [LinkedIn Post Editor] Task output: # Unlocking the Power of Transformer Models 🌐\n",
            "\n",
            "## 🌟 Revolutionizing AI and NLP: An In-depth Exploration 🌟\n",
            "\n",
            "Get ready to explore the fascinating realm of Transformer models, a technology that has become a cornerstone in AI and natural language processing (NLP). Today, we dive into the world of Transformers, shedding light on their impact and potential. 🚀\n",
            "\n",
            "## 📰 Industry Trends and Influencers 📰\n",
            "\n",
            "**🔑 The Seminal Paper:**\n",
            "It all started with a groundbreaking publication by Vaswani et. al. (2017), titled \"Attention is All You Need.\" This paper introduced the Transformer architecture, a paradigm shift in sequence transduction tasks, and set the stage for modern NLP advancements. 📚\n",
            "\n",
            "**🤗 Hugging Face: A Community Catalyst:**\n",
            "Hugging Face, a prominent NLP company, has been instrumental in democratizing Transformer models. Their platform provides a wealth of pre-trained models, datasets, and a vibrant community, making it a go-to resource for developers and researchers worldwide. 🌐\n",
            "\n",
            "**💊 Healthcare Impact:**\n",
            "Transformers have made significant strides in healthcare, as evidenced by a study from Brown et al. (2021). The model exhibited exceptional accuracy in medical diagnosis and tailored treatment planning, demonstrating its real-world value. 🩺\n",
            "\n",
            "## 🧠 The Transformer's Evolution 🧠\n",
            "\n",
            "The Transformer architecture has evolved significantly since its inception. Here's a glimpse into its journey:\n",
            "- **🌟 The Original Transformer:** Vaswani et al.'s paper introduced the self-attention mechanism and encoder-decoder architecture, marking a departure from conventional models.\n",
            "- **🦾 Advancements:** Researchers have continually improved Transformers with techniques like multi-head attention, positional encoding, and optimized training, resulting in enhanced performance.\n",
            "\n",
            "## 📈 Real-World Success Stories 📈\n",
            "\n",
            "🌟 **Healthcare Revolution:** Transformer models have proven invaluable in healthcare, facilitating early disease detection and personalized treatment plans.\n",
            "💰 **Financial Security:** In finance, these models excel at fraud detection and risk assessment, safeguarding transactions.\n",
            "🌐 **Global Communication:** Real-time language translation services have been transformed, fostering global connectivity.\n",
            "\n",
            "## 💻 Technical Insights: Understanding Transformers 💻\n",
            "\n",
            "The core of Transformer models is the self-attention mechanism, enabling the model to prioritize input words in a sentence. This mechanism is implemented through the encoder-decoder architecture:\n",
            "\n",
            "```\n",
            "Encoder: Encodes input data, creating a contextual representation.\n",
            "Decoder: Decodes the encoded data, generating the desired output.\n",
            "```\n",
            "\n",
            "![Transformer Model Architecture](transformer_architecture.png)\n",
            "\n",
            "## 🌟 Navigating Challenges and the Future 🌟\n",
            "\n",
            "Despite their success, Transformer models face challenges:\n",
            "- **🤔 Ethical Responsibility:** Ensuring ethical AI development and mitigating biases are critical considerations.\n",
            "- **🔬 Research Focus:** Ongoing research aims to improve efficiency, tackle overfitting, and explore innovative architectures.\n",
            "\n",
            "## 🌈 Engage and Learn 🌈\n",
            "\n",
            "🔍 **Resources:**\n",
            "- [Attention is All You Need](https://arxiv.org/abs/1706.03762) - The seminal paper.\n",
            "- [Hugging Face Platform](https://huggingface.co/) - A hub for pre-trained models and datasets.\n",
            "- [Transformers in Healthcare](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8121279/) - A review of healthcare applications.\n",
            "\n",
            "💬 **Join the Discussion:**\n",
            "- What are your experiences with Transformer models?\n",
            "- How do you envision Transformers influencing your industry's future?\n",
            "- Share your insights and questions in the comments! Let's foster a learning environment! 🌱\n",
            "\n",
            "## 📌 Connect and Explore 📌\n",
            "#TransformerModels #NLP #AIInnovation #HuggingFace #SelfAttention #TechnicalDeepDive #DataScience #EthicalAI\n",
            "\n",
            "---\n",
            "Let's explore the potential of Transformer models together! This post aims to inspire and educate, fostering a community of AI and NLP enthusiasts. Engage, share your experiences, and contribute to meaningful discussions! 💡🤝\n",
            "\n",
            "---\n",
            "Stay tuned for more insightful content on your LinkedIn feed! 🌐 #AICommunity #KnowledgeSharing\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: LinkedIn Post Humanizer\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Transform the provided blog post into a LinkedIn post with a natural, conversational tone. \n",
            "1. Ensure the content feels authentic and personal, resonating with a professional audience.\n",
            "2. Reframe the content to encourage engagement and spark discussions, while maintaining factual accuracy.\n",
            "3. Avoid direct replication of the blog post to ensure originality and prevent plagiarism.\n",
            "4. Highlight key insights and include relevant hashtags and calls to action for maximum reach.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
            "\n",
            "Final Answer: # Unlocking the Power of #TransformerModels\n",
            "\n",
            "🌟 Are you ready to dive into the world of AI and NLP? 🌟\n",
            "\n",
            "Today, I want to share my excitement about a technology that has been making waves in the industry: Transformer models! These innovative models have revolutionized the way we approach natural language processing and are set to transform various sectors.\n",
            "\n",
            "## 📰 A Journey Begins with a Seminal Paper 📰\n",
            "It all began with a brilliant publication by Vaswani et al., which introduced the concept of 'Attention' and changed the game for sequence transduction tasks. This paper laid the foundation for the incredible advancements we see today! 📝\n",
            "\n",
            "## 🤗 Hugging Face: Democratizing AI 🤗\n",
            "Hugging Face, a community favorite, has played a significant role in making Transformer models accessible. Their platform offers a treasure trove of pre-trained models and datasets, empowering developers and researchers to experiment and innovate. 🛠️\n",
            "\n",
            "## 🩺 Healthcare's New Ally 🩺\n",
            "One of the most remarkable impacts of Transformers is in healthcare. Imagine a model that can accurately diagnose medical conditions and suggest personalized treatments! A study by Brown et al. showcases how Transformers are saving lives and improving patient care. 🌡️\n",
            "\n",
            "## 🌱 Evolution and Real-World Success 🌱\n",
            "The Transformer architecture has evolved, becoming more powerful and versatile. From healthcare to finance and global communication, these models excel at various tasks:\n",
            "- **Healthcare:** Early disease detection and personalized medicine.\n",
            "- **Finance:** Securing transactions with fraud detection.\n",
            "- **Language Translation:** Enabling real-time global conversations.\n",
            "\n",
            "## 💡 Technical Deep Dive: Self-Attention 💡\n",
            "The secret sauce of Transformers? The self-attention mechanism! It allows the model to focus on relevant words in a sentence, ensuring contextually rich outputs. This is achieved through the encoder-decoder architecture, a true engineering marvel! 💻\n",
            "\n",
            "## 🌟 Challenges and the Road Ahead 🌟\n",
            "As we embrace the potential of Transformers, we must also address challenges. Ethical AI development and bias mitigation are essential. Additionally, ongoing research focuses on improving efficiency and exploring new architectures. 🌅\n",
            "\n",
            "## 🌈 Let's Chat and Learn Together! 🌈\n",
            "- Have you worked with Transformer models? What was your experience like?\n",
            "- How do you see Transformers shaping your industry's future?\n",
            "- Let's engage in a meaningful discussion! Share your thoughts, insights, and questions in the comments. Let's learn and grow together! 🌱\n",
            "\n",
            "Remember, the AI and NLP community thrives on knowledge sharing. Stay tuned for more insightful content, and let's keep the conversation going! 📢\n",
            "\n",
            "#AIInnovation #NLPBreakthroughs #HuggingFaceCommunity #SelfAttentionMechanism #HealthcareTech #DataScience #EthicalAI #AICommunity\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [LinkedIn Post Humanizer] Task output: # Unlocking the Power of #TransformerModels\n",
            "\n",
            "🌟 Are you ready to dive into the world of AI and NLP? 🌟\n",
            "\n",
            "Today, I want to share my excitement about a technology that has been making waves in the industry: Transformer models! These innovative models have revolutionized the way we approach natural language processing and are set to transform various sectors.\n",
            "\n",
            "## 📰 A Journey Begins with a Seminal Paper 📰\n",
            "It all began with a brilliant publication by Vaswani et al., which introduced the concept of 'Attention' and changed the game for sequence transduction tasks. This paper laid the foundation for the incredible advancements we see today! 📝\n",
            "\n",
            "## 🤗 Hugging Face: Democratizing AI 🤗\n",
            "Hugging Face, a community favorite, has played a significant role in making Transformer models accessible. Their platform offers a treasure trove of pre-trained models and datasets, empowering developers and researchers to experiment and innovate. 🛠️\n",
            "\n",
            "## 🩺 Healthcare's New Ally 🩺\n",
            "One of the most remarkable impacts of Transformers is in healthcare. Imagine a model that can accurately diagnose medical conditions and suggest personalized treatments! A study by Brown et al. showcases how Transformers are saving lives and improving patient care. 🌡️\n",
            "\n",
            "## 🌱 Evolution and Real-World Success 🌱\n",
            "The Transformer architecture has evolved, becoming more powerful and versatile. From healthcare to finance and global communication, these models excel at various tasks:\n",
            "- **Healthcare:** Early disease detection and personalized medicine.\n",
            "- **Finance:** Securing transactions with fraud detection.\n",
            "- **Language Translation:** Enabling real-time global conversations.\n",
            "\n",
            "## 💡 Technical Deep Dive: Self-Attention 💡\n",
            "The secret sauce of Transformers? The self-attention mechanism! It allows the model to focus on relevant words in a sentence, ensuring contextually rich outputs. This is achieved through the encoder-decoder architecture, a true engineering marvel! 💻\n",
            "\n",
            "## 🌟 Challenges and the Road Ahead 🌟\n",
            "As we embrace the potential of Transformers, we must also address challenges. Ethical AI development and bias mitigation are essential. Additionally, ongoing research focuses on improving efficiency and exploring new architectures. 🌅\n",
            "\n",
            "## 🌈 Let's Chat and Learn Together! 🌈\n",
            "- Have you worked with Transformer models? What was your experience like?\n",
            "- How do you see Transformers shaping your industry's future?\n",
            "- Let's engage in a meaningful discussion! Share your thoughts, insights, and questions in the comments. Let's learn and grow together! 🌱\n",
            "\n",
            "Remember, the AI and NLP community thrives on knowledge sharing. Stay tuned for more insightful content, and let's keep the conversation going! 📢\n",
            "\n",
            "#AIInnovation #NLPBreakthroughs #HuggingFaceCommunity #SelfAttentionMechanism #HealthcareTech #DataScience #EthicalAI #AICommunity\n",
            "\n",
            "\u001b[00m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "Markdown(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "id": "pCU-K56cih9e",
        "outputId": "1e7c7ed5-e7d4-4be3-b2f2-0410ec143ed8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Unlocking the Power of #TransformerModels\n\n🌟 Are you ready to dive into the world of AI and NLP? 🌟\n\nToday, I want to share my excitement about a technology that has been making waves in the industry: Transformer models! These innovative models have revolutionized the way we approach natural language processing and are set to transform various sectors.\n\n## 📰 A Journey Begins with a Seminal Paper 📰\nIt all began with a brilliant publication by Vaswani et al., which introduced the concept of 'Attention' and changed the game for sequence transduction tasks. This paper laid the foundation for the incredible advancements we see today! 📝\n\n## 🤗 Hugging Face: Democratizing AI 🤗\nHugging Face, a community favorite, has played a significant role in making Transformer models accessible. Their platform offers a treasure trove of pre-trained models and datasets, empowering developers and researchers to experiment and innovate. 🛠️\n\n## 🩺 Healthcare's New Ally 🩺\nOne of the most remarkable impacts of Transformers is in healthcare. Imagine a model that can accurately diagnose medical conditions and suggest personalized treatments! A study by Brown et al. showcases how Transformers are saving lives and improving patient care. 🌡️\n\n## 🌱 Evolution and Real-World Success 🌱\nThe Transformer architecture has evolved, becoming more powerful and versatile. From healthcare to finance and global communication, these models excel at various tasks:\n- **Healthcare:** Early disease detection and personalized medicine.\n- **Finance:** Securing transactions with fraud detection.\n- **Language Translation:** Enabling real-time global conversations.\n\n## 💡 Technical Deep Dive: Self-Attention 💡\nThe secret sauce of Transformers? The self-attention mechanism! It allows the model to focus on relevant words in a sentence, ensuring contextually rich outputs. This is achieved through the encoder-decoder architecture, a true engineering marvel! 💻\n\n## 🌟 Challenges and the Road Ahead 🌟\nAs we embrace the potential of Transformers, we must also address challenges. Ethical AI development and bias mitigation are essential. Additionally, ongoing research focuses on improving efficiency and exploring new architectures. 🌅\n\n## 🌈 Let's Chat and Learn Together! 🌈\n- Have you worked with Transformer models? What was your experience like?\n- How do you see Transformers shaping your industry's future?\n- Let's engage in a meaningful discussion! Share your thoughts, insights, and questions in the comments. Let's learn and grow together! 🌱\n\nRemember, the AI and NLP community thrives on knowledge sharing. Stay tuned for more insightful content, and let's keep the conversation going! 📢\n\n#AIInnovation #NLPBreakthroughs #HuggingFaceCommunity #SelfAttentionMechanism #HealthcareTech #DataScience #EthicalAI #AICommunity"
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "byxfjJiNv5nH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}